{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 01 - Processing the data\n",
    "\n",
    "   * In this step we shall read the input words - dinosaurs names as each setence with '\\n' char at the end of each name we also create our vocabulary characters list\n",
    "\n",
    "   * We need to maintain two hashtable char_to-index and index_to_char each of these are mappings of characters in vocabulary to an index and viceversa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data = open('IndianNames.txt', 'r').read()\n",
    "input_data = input_data.lower()\n",
    "\n",
    "unique_chars = list(set(input_data))\n",
    "input_data_size, voacb_chars_size = len(input_data), len(unique_chars)\n",
    "unique_chars = sorted(unique_chars)\n",
    "\n",
    "char_to_index = { ch:ind for ind,ch in enumerate(unique_chars) } #Hashtable mapping char -> index\n",
    "index_to_char = { ind:ch for ind,ch in enumerate(unique_chars) } #Hashtable mapping index -> char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 02 - Building the model\n",
    "\n",
    "Building the model include the following:\n",
    "\n",
    "    a) Training the model\n",
    "    \n",
    "    b) Optimizing:\n",
    "    \n",
    "        b.1) Forward Prop\n",
    "        b.2) Backward Prop\n",
    "        b.3) Gradient Clipping\n",
    "        b.4) Parameters updation\n",
    "        \n",
    "    c) Performing sampling -> to ensure the training.\n",
    "\n",
    "## 2.1 - Gradient Clipping\n",
    "\n",
    "Gradient clipping is performed to avoid the exploding gradients problem, in which the gradients suffer from having extremely high values.\n",
    "\n",
    "Hence, given a range [-n, n] check if the value is within the range if:\n",
    "\n",
    "    a) num > n => num = n\n",
    "    \n",
    "    b) num < -n => num = -n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clip(gradients, maxValue):\n",
    "    \n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "    \n",
    "    for ele in [dWaa, dWax, dWya, db, dby]:\n",
    "        np.clip(ele, -1 * maxValue, maxValue, out=ele)\n",
    "    \n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Sampling\n",
    "\n",
    "Sampling the model is assuming that the model is trained and is ready to generate the next character and there by dinosaur name \n",
    "\n",
    "when it is fed by an input character. Problem here would be only one name will be generated for one character. Here are the \n",
    "\n",
    "steps for sampling:\n",
    "\n",
    "**Step 01**: Retrieve the parameters into respective variables, and initalize the $x^{\\langle 1 \\rangle} = \\vec{0}$ and \n",
    "\n",
    "$a^{\\langle 0 \\rangle} = \\vec{0}$\n",
    "\n",
    "**Step 02**: Calculate the $a^{\\langle 1 \\rangle}$ and $y^{\\langle 1 \\rangle}$, equations required are:\n",
    "\n",
    "$$ a^{\\langle t+1 \\rangle} = \\tanh(W_{ax}  x^{\\langle t+1 \\rangle } + W_{aa} a^{\\langle t \\rangle } + b)\\tag{1}$$\n",
    "\n",
    "$$ z^{\\langle t + 1 \\rangle } = W_{ya}  a^{\\langle t + 1 \\rangle } + b_y \\tag{2}$$\n",
    "\n",
    "$$ \\hat{y}^{\\langle t+1 \\rangle } = softmax(z^{\\langle t + 1 \\rangle })\\tag{3}$$\n",
    "\n",
    "**Note:** The vectors **x** and **a** are 2D not 1D.\n",
    "\n",
    "**Step 03**: Sampling, the $y^$ which is a result of softmax probability vector. If the highest probability index is concerned \n",
    "\n",
    "everytime there shall be a problem of having single name for a input character. For this case, we shall consider random function \n",
    "\n",
    "np.random.choice().\n",
    "\n",
    "**Step 04**: Updating the $x^{\\langle t \\rangle}$ with the values of $x^{\\langle t+1 \\rangle}$ by initalizing all the values to \n",
    "\n",
    "zero and 1 to idx index indicating one-hot vector of that character.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(parameters, char_to_ix, seed):\n",
    "    \n",
    "    # Retrieve parameters\n",
    "    \n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "\n",
    "    # Initalize x and a_prev with zeros\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # Indices [] is the empty list, to which we append the index of the characters the model is giving.\n",
    "    indices = []\n",
    "    \n",
    "    # this the number at every step which holds the index of the next character.\n",
    "    idx = -1 \n",
    "    \n",
    "    # Looping over t number of time_steps, counter == 50 or when the next index is '\\n' \n",
    "    counter = 0\n",
    "    newline_character = char_to_ix['\\n']\n",
    "    \n",
    "    while (idx != newline_character and counter != 50):\n",
    "        \n",
    "        # Three equations in step:02 - f.w.d propogation\n",
    "        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)\n",
    "        z = np.dot(Wya, a) + by\n",
    "        y = softmax(z)\n",
    "        \n",
    "        np.random.seed(counter+seed) \n",
    "        \n",
    "        # sampling the index values out of the probabilities in y\n",
    "        idx = np.random.choice(range(len(y)), p=y[:, 0])\n",
    "\n",
    "        # Append the index to \"indices[]\"\n",
    "        indices.append(idx)\n",
    "        \n",
    "        # Step-04\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[idx] = 1\n",
    "        \n",
    "        # Update \"a_prev\" to be \"a\"\n",
    "        a_prev = a\n",
    "        \n",
    "        seed += 1\n",
    "        counter +=1\n",
    "\n",
    "    if (counter == 50):\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Optimizing the Model\n",
    "\n",
    "\n",
    "In this step we perform F.W.D propogation, B.W.D propogation, gradient Clipping and update the parameters\n",
    "\n",
    "a) Perform F.W.D prop -> rnn_fwd_prop() method defined in utils\n",
    "\n",
    "b) Perform B.W.D prop -> rnn_bck_prop() method defined in utils\n",
    "\n",
    "c) Perform gradient clipping defined in step 2.1\n",
    "\n",
    "d) Update the parameters -> update_parameters() method defined in utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
    "        \n",
    "    # a) Forward propagate \n",
    "    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n",
    "    \n",
    "    # b) Backpropagate\n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
    "    \n",
    "    # Gradient clipping with range [-5, 5]\n",
    "    gradients = clip(gradients, 5)\n",
    "    \n",
    "    # Update parameters\n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "    \n",
    "    return loss, gradients, a[len(X)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 - Training the Model\n",
    "\n",
    "\n",
    "a) In this we step we shall initialize the parameters -> Wax (n_x, n_a), Waa (n_a, n_a), Wya (n_y, n_a), b (n_a, 1), by(n_y, 1).\n",
    "\n",
    "b) Calculate the initial loss. This is to ensure that we give a smooth calculation at every step where loss is calculated.\n",
    "\n",
    "c) Prepare a list of dinosaurs name by stripping the '\\n'. And shuffle this list.\n",
    "\n",
    "d) Initialize a_prev (n_a, 1) with zeros.\n",
    "\n",
    "e) Run the optimization loop over given number of iterations.\n",
    "\n",
    "    e.1) Retrieve each example & create a list of the char's in each example & map these char's to index using char_to_index.\n",
    "   \n",
    "    e.2) Initalize X to [None] + above create list of indices of the example.\n",
    "    \n",
    "    e.3) Initalize Y to indices list of example + char_to_index('\\n').\n",
    "    \n",
    "    e.4) Optimize the values (F.W.D Prop -> B.W.D Prop -> Gradient_Clipping -> Update_Parameters)\n",
    "    \n",
    "    e.5) Perform sampling for every 2000 iterations to check if the model is training properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(data, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27, verbose = False):\n",
    "\n",
    "    \n",
    "    # x.shape(n_x, m, t_x), n_x = n_y = vocab_size\n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "    \n",
    "    # Calculate initial loss, which helps in smoothing the loss in further time steps\n",
    "    loss = get_initial_loss(vocab_size, dino_names)\n",
    "    \n",
    "    # Examples[] is the list of dino_names\n",
    "    with open(\"IndianNames.txt\") as f:\n",
    "        examples = f.readlines()\n",
    "    examples = [x.lower().strip() for x in examples]\n",
    "    \n",
    "    # Random shuffle of examples (.seed() initialize the random number generator)\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(examples)\n",
    "    \n",
    "    # Initalize the previous activation to zeros.\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # Optimization loop\n",
    "    for j in range(num_iterations):\n",
    "        \n",
    "        \n",
    "        #idx will be the index of character in example.\n",
    "        idx = j % len(examples)\n",
    "        \n",
    "        # Set the input X to [None] + list of indices of characters \n",
    "        X = [None] + [char_to_ix[c] for c in examples[idx]]\n",
    "        \n",
    "        # Set the labels Y \n",
    "        ix_newline = char_to_ix['\\n']\n",
    "        Y = X[1:] + [ix_newline]\n",
    "\n",
    "        # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters\n",
    "        # Choose a learning rate of 0.01\n",
    "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters)\n",
    "        \n",
    "        # debug statements to aid in correctly forming X, Y\n",
    "        if verbose and j in [0, len(examples) -1, len(examples)]:\n",
    "            print(\"j = \" , j, \"idx = \", idx,) \n",
    "            \n",
    "        if verbose and j in [0]:\n",
    "            print(\"single_example =\", examples[idx])\n",
    "            print(\"single_example_ix\", X[1:])\n",
    "            print(\" X = \", X, \"\\n\", \"Y =       \", Y, \"\\n\")\n",
    "        \n",
    "        # Use a latency trick to keep the loss smooth. It happens here to accelerate the training.\n",
    "        loss = smooth(loss, curr_loss)\n",
    "\n",
    "        # Every 2000 Iteration, generate \"n\" characters thanks to sample() to check if the model is learning properly\n",
    "        if j % 2000 == 0:\n",
    "            \n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "            \n",
    "            # The number of dinosaur names to print\n",
    "            seed = 0\n",
    "            for name in range(dino_names):\n",
    "                \n",
    "                # Sample indices and print them\n",
    "                sampled_indices = sample(parameters, char_to_ix, seed)\n",
    "                print_sample(sampled_indices, ix_to_char)\n",
    "                \n",
    "                seed += 1  # To get the same result (for grading purposes), increment the seed by one. \n",
    "      \n",
    "            print('\\n')\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j =  0 idx =  0\n",
      "single_example = rachana\n",
      "single_example_ix [18, 1, 3, 8, 1, 14, 1]\n",
      " X =  [None, 18, 1, 3, 8, 1, 14, 1] \n",
      " Y =        [18, 1, 3, 8, 1, 14, 1, 0] \n",
      "\n",
      "Iteration: 0, Loss: 23.074155\n",
      "\n",
      "Nkzxwtdmeqoeyhsqwasjjjvu\n",
      "Kneb\n",
      "Kzxwtdmeqoeyhsqwasjjjvu\n",
      "Neb\n",
      "Zxwtdmeqoeyhsqwasjjjvu\n",
      "Eb\n",
      "Xwtdmeqoeyhsqwasjjjvu\n",
      "\n",
      "\n",
      "Iteration: 2000, Loss: 20.366060\n",
      "\n",
      "Ilvspoan\n",
      "Gha\n",
      "Hutsibhariavaris\n",
      "Ic\n",
      "Wsrkan\n",
      "A\n",
      "Tsibhariavaris\n",
      "\n",
      "\n",
      "Iteration: 4000, Loss: 18.508251\n",
      "\n",
      "Laxtrkakamhavanisanadesm\n",
      "Hej\n",
      "Iytso\n",
      "La\n",
      "Xutn\n",
      "Ca\n",
      "Utn\n",
      "\n",
      "\n",
      "Iteration: 6000, Loss: 17.703618\n",
      "\n",
      "Nevtrdan\n",
      "Jika\n",
      "Kutrm\n",
      "Nad\n",
      "Wutma\n",
      "Gaahnra\n",
      "Tushh\n",
      "\n",
      "\n",
      "j =  6469 idx =  6469\n",
      "j =  6470 idx =  0\n",
      "Iteration: 8000, Loss: 16.949048\n",
      "\n",
      "Mayrla\n",
      "Jeed\n",
      "Kutrea\n",
      "Mad\n",
      "Wrppag\n",
      "Da\n",
      "Surak\n",
      "\n",
      "\n",
      "Iteration: 10000, Loss: 16.758561\n",
      "\n",
      "Nhutsl\n",
      "Khad\n",
      "Lutti\n",
      "Nad\n",
      "Vuto\n",
      "Faajlo\n",
      "Suram\n",
      "\n",
      "\n",
      "Iteration: 12000, Loss: 16.558236\n",
      "\n",
      "Onvisi\n",
      "Mika\n",
      "Mussi\n",
      "Oid\n",
      "Votpal\n",
      "Gaakji\n",
      "Sushi\n",
      "\n",
      "\n",
      "Iteration: 14000, Loss: 16.369947\n",
      "\n",
      "Mevoti\n",
      "Kika\n",
      "Kussi\n",
      "Maeabisa\n",
      "Voti\n",
      "Eea\n",
      "Suram\n",
      "\n",
      "\n",
      "Iteration: 16000, Loss: 16.336368\n",
      "\n",
      "Nevtun\n",
      "Khad\n",
      "Kusnmagama\n",
      "Nabafira\n",
      "Vrusghana\n",
      "Daajmpa\n",
      "Suramandat\n",
      "\n",
      "\n",
      "Iteration: 18000, Loss: 16.176349\n",
      "\n",
      "Onvoti\n",
      "Kika\n",
      "Lussmam\n",
      "Oja\n",
      "Vrushabhadtal\n",
      "Gaajli\n",
      "Suran\n",
      "\n",
      "\n",
      "Iteration: 20000, Loss: 16.203992\n",
      "\n",
      "Mevsun\n",
      "Khad\n",
      "Kussi\n",
      "Madafesh\n",
      "Vitra\n",
      "Gaajit\n",
      "Suram\n",
      "\n",
      "\n",
      "Iteration: 22000, Loss: 16.256409\n",
      "\n",
      "Nevrin\n",
      "Kika\n",
      "Kusmi\n",
      "Nababir\n",
      "Voti\n",
      "Gaairma\n",
      "Supan\n",
      "\n",
      "\n",
      "Iteration: 24000, Loss: 15.992449\n",
      "\n",
      "Mevrni\n",
      "Kila\n",
      "Kusti\n",
      "Magagri\n",
      "Voural\n",
      "Gaajli\n",
      "Suramala\n",
      "\n",
      "\n",
      "Iteration: 26000, Loss: 16.137616\n",
      "\n",
      "Nevosm\n",
      "Khae\n",
      "Musti\n",
      "Naf\n",
      "Vito\n",
      "Gaahisa\n",
      "Suram\n",
      "\n",
      "\n",
      "Iteration: 28000, Loss: 16.178596\n",
      "\n",
      "Ngusti\n",
      "Kika\n",
      "Kusti\n",
      "Naebarsa\n",
      "Viural\n",
      "Faajko\n",
      "Surbi\n",
      "\n",
      "\n",
      "Iteration: 30000, Loss: 15.894756\n",
      "\n",
      "Nazush\n",
      "Kila\n",
      "Lussnal\n",
      "Nagajhi\n",
      "Vkuram\n",
      "Faajpra\n",
      "Suram\n",
      "\n",
      "\n",
      "Iteration: 32000, Loss: 15.944441\n",
      "\n",
      "Nevjisa\n",
      "Mija\n",
      "Musmi\n",
      "Nafahmi\n",
      "Vishan\n",
      "Gaaira\n",
      "Suran\n",
      "\n",
      "\n",
      "Iteration: 34000, Loss: 15.978308\n",
      "\n",
      "Nevror\n",
      "Moha\n",
      "Mustlal\n",
      "Nah\n",
      "Votral\n",
      "Gaajri\n",
      "Suram\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = model(input_data, index_to_char, char_to_index, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
